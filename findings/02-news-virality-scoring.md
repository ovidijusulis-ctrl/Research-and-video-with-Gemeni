# #2 — News Virality Scoring & Trending APIs

**Priority:** Critical | **Used in:** QO backlog Priority 6 (news-fetcher.js)
**Status:** Pending

---

## Prompt for Gemini Deep Research

Copy everything below this line into Gemini Deep Research:

---

Research how to build an automated news virality scoring system for selecting the most engaging stories for a daily English learning podcast.

Context: I run "Ovi English School" — a daily automated podcast that fetches news via RSS and turns it into English lessons. Problem: random news selection means boring episodes. I need to automatically pick stories that are trending and interesting.

Research:

1. TRENDING/VIRAL DETECTION APIs (free or cheap)
   - Google Trends API — how to use it, rate limits, Node.js integration
   - Twitter/X trending topics API
   - Reddit trending — r/worldnews, r/technology, r/todayilearned
   - NewsAPI.org, GNews API, or similar aggregators with "popularity" signals
   - Any free APIs that score articles by engagement/shares

2. VIRALITY SCORING ALGORITHM
   - What factors predict a story will be interesting?
   - How do news aggregators (Google News, Apple News, Flipboard) rank stories?
   - NLP-based headline scoring — what words/patterns predict engagement?
   - Celebrity/famous person detection in headlines
   - Numbers and statistics in headlines (research shows these get more clicks)

3. NEWS SOURCES FOR LANGUAGE LEARNERS
   - What RSS feeds provide engaging, globally-relevant news?
   - Which sources write at accessible English levels?
   - Best entertainment/lifestyle/tech sources (more engaging than politics)
   - Regional sources for Japanese audience specifically

4. IMPLEMENTATION APPROACH
   - Node.js code examples for fetching Google Trends
   - How to score headlines using simple NLP (no ML training needed)
   - Combining multiple signals into a single "interest score"
   - Caching and rate limiting strategies

5. CONTENT FILTERING
   - How to automatically filter out boring/dry stories
   - How to detect "clickbait" vs genuinely interesting
   - Topic diversity — ensuring variety across days (not all politics, not all tech)

I need practical, implementable solutions with code examples in Node.js. Budget: free tier APIs only.

---

## Research Results

Automated Virality: Engineering a Node.js System for ESL Content SelectionExecutive Architecture and System DesignThe convergence of algorithmic content curation and educational technology presents a unique engineering challenge: identifying news stories that possess high viral potential while simultaneously adhering to strict pedagogical constraints for English as a Second Language (ESL) learners. The objective is to build an automated system using Node.js that ingests real-time data, analyzes social engagement signals, and filters content based on linguistic complexity. This report details the comprehensive architectural, algorithmic, and content sourcing strategies required to construct such a system, ensuring that the selected stories are not only popular but also educationally valuable.Node.js serves as the optimal runtime environment for this architecture due to its non-blocking, event-driven nature, which is essential for handling the high concurrency required by multi-source data ingestion. A system tasked with monitoring the "pulse" of the internet must process thousands of signals simultaneously—from RSS feed updates to real-time API streams—without blocking the execution thread. The ecosystem's rich library of Natural Language Processing (NLP) tools and API wrappers further solidifies its suitability for this application. The proposed system operates on a cyclical pipeline: ingestion of raw signals, enrichment through social metric analysis, linguistic evaluation, and finally, a weighted scoring mechanism that outputs a curated playlist for podcast production.The Event-Driven Ingestion LayerThe foundation of the system is the ingestion layer, which must be resilient to network latency and third-party rate limiting. In a Node.js environment, this is typically managed through a task orchestration library such as node-cron or a message queue like Bull (backed by Redis). These tools allow the system to decouple the fetching of data from its processing, ensuring that a failure in one RSS feed or a timeout from the Google Trends API does not cascade into a system-wide failure. The architecture must support distinct ingestion intervals: "fast" lanes for real-time trend detection (e.g., every 15 minutes) and "slow" lanes for archival or deep-dive content (e.g., every 6 hours).This separation of concerns is critical when dealing with the volatile nature of viral news. A story can spike in popularity within minutes, necessitating a system that can react swiftly. Conversely, the linguistic analysis—tokenizing text, calculating Flesch-Kincaid scores, and extracting named entities—is computationally intensive and benefits from the asynchronous processing model of Node.js. By utilizing worker threads or separate microservices for the NLP tasks, the main event loop remains free to handle incoming network requests, maintaining high throughput for data acquisition.Real-Time Trend Detection and the Google Trends EcosystemTo identify "engaging" stories, one must first quantify public interest. Google Trends remains the gold standard for this metric, offering insights into what the world is searching for at any given moment. However, integrating this data into a programmatic environment is fraught with challenges due to the historical lack of a stable, public API and Google's aggressive bot detection measures.Navigating the API Landscape: Official vs. Community SolutionsFor years, the Node.js community relied on unofficial wrappers like google-trends-api to scrape data from the Google Trends backend. However, as of late 2024 and 2025, the stability of these libraries has degraded significantly. Developers frequently report HTTP 429 "Too Many Requests" errors and 404 responses, driven by changes in Google's frontend architecture and enhanced security protocols. The google-trends-api package, once the industry standard, is now deprecated in favor of newer forks like @alkalisummer/google-trends-js, which have been updated to align with the latest endpoint signatures.The introduction of an official Google Trends API in an Alpha testing phase marks a pivotal shift in this landscape. Unlike the web interface, which scales data from 0 to 100 based on the highest point in a specific query's timeframe, the official API promises "consistently scaled" data. This means the indices returned are comparable across different requests and time periods, a feature that allows for true longitudinal analysis. For a virality scoring system, this is transformative; it enables the comparison of the absolute magnitude of a trend today versus a trend from last week, rather than just relative popularity within a single isolated query.MethodReliabilityData ConsistencyAccess BarrierOfficial Alpha APIHighExcellent (Cross-request scaling)Very High (Invite only) Legacy google-trends-apiLowGood (Relative scaling)Low (Public NPM) @alkalisummer ForkModerateGoodLow (Public NPM) Puppeteer ScrapingModerate/HighExact (Visual match)High (Resource intensive) Engineering Workarounds for Rate LimitsUntil the official API becomes widely available, developers must implement robust workarounds to bypass the "Too Many Requests" (429) errors that plague scraping solutions. The root cause of these errors is often the lack of valid browser credentials in the request headers. A sophisticated Node.js scraper cannot simply use axios or fetch; it must mimic a legitimate user session.One effective strategy involves using browser automation tools like Puppeteer with the puppeteer-extra-plugin-stealth. This plugin obscures the automated nature of the browser by overriding default Navigator properties that typically reveal a "headless" state (e.g., navigator.webdriver). By launching a controlled browser instance, navigating to Google Trends, and extracting the cookies and authentication tokens, the system can inject these credentials into subsequent API calls, significantly reducing the likelihood of being blocked.Furthermore, developers have found success by manually extracting cURL commands from a logged-in browser session and porting the headers—specifically the Cookie and User-Agent strings—into their Node.js application. This method essentially "borrows" the trust level of a real user account. However, this is a fragile solution; session tokens expire, requiring a mechanism to refresh authentication automatically, potentially by rotating through a pool of Google accounts or using a proxy service to distribute the request load across multiple IP addresses.Content Aggregation: The News API EcosystemOnce a trend is identified (e.g., "Solar Eclipse"), the system must retrieve high-quality articles related to that topic. The choice of News API is dictated by three factors: the breadth of sources, the depth of metadata (sentiment, social signals), and cost efficiency. For a podcast system, the ability to filter by language ("English") and exclude diverse content types (e.g., removing "opinion" pieces to focus on "factual" reporting) is essential.Comparative Analysis of News ProvidersNewsData.io offers a compelling balance for this use case, providing access to over 88,000 sources in 80 languages. Its free tier allows for 200 credits per day, which is sufficient for a daily podcast workflow if queries are batched efficiently. Crucially, it permits commercial use, a rarity among free tiers, and allows for granular filtering by category (e.g., "Technology," "Science") which helps in pre-selecting genres that perform well with learners.NewsAPI.org, while popular, has stricter limitations. Its free plan is limited to non-commercial use and provides data with a delay, which can be detrimental when chasing "viral" stories that rely on immediacy. Furthermore, it lacks the advanced sentiment analysis features found in premium competitors.Webz.io (formerly Webhose) distinguishes itself by providing "social signals" directly in the API response. This means that for every article, the API returns the number of Facebook shares, Pinterest pins, and other engagement metrics. This data is invaluable for the virality scoring algorithm, as it provides a ground-truth measurement of an article's performance without requiring a secondary scrape of social media platforms. However, the free tier is limited, often necessitating a hybrid approach where a cheaper API (like NewsData.io) is used for bulk discovery, and Webz.io is used for targeted enrichment of high-potential candidates.RSS Feeds as Targeted Ingestion VectorsBeyond general aggregators, direct RSS ingestion from curated sources ensures a steady stream of high-quality, niche content. The rss-parser library in Node.js is the standard tool for this, allowing for the transformation of XML feeds into JSON objects.For an English learning podcast, "Weird News" and "Good News" are particularly effective genres. They often feature simple narrative structures and engaging, novelty-driven vocabulary.Strange News Sources: Feeds from Sky News Oddities, AP News Oddities, and Oddity Central provide stories about "drunk raccoons" or "unusual robberies". These stories have inherent viral potential due to their absurdity and are excellent for teaching idioms and descriptive adjectives.Uplifting Content: The Good News Network and DailyGood offer feeds focused on resilience and scientific breakthroughs. These stories evoke positive emotions, creating a safe learning environment that encourages retention.Japan and Asia Context: For learners specifically interested in Asian culture, feeds from SoraNews24, Japan Today, and The Mainichi are critical. These sources publish English-language stories about Japanese trends, anime, and food, bridging the cultural gap for learners and providing high engagement through cultural relevance.To effectively parse these feeds, the rss-parser must be configured to extract custom fields. Standard RSS fields (title, link, pubDate) are often insufficient. Many feeds include full content or specific image URLs in custom namespaces (e.g., content:encoded or media:content). Extending the parser to capture these fields ensures that the NLP pipeline has the full text of the article to analyze, rather than just a brief summary.Social Signal Intelligence: The Reddit VectorWhile News APIs provide the content, social platforms provide the validation. Reddit is arguably the most potent predictor of viral potential on the open web. A story that rises to the top of r/worldnews, r/technology, or r/todayilearned is almost guaranteed to gain traction elsewhere. Therefore, a specialized module for monitoring Reddit is a non-negotiable component of the system.Architecting with SnoowrapThe snoowrap library is the definitive Node.js wrapper for the Reddit API, offering a Promise-based interface that simplifies the complex OAuth interactions required to access Reddit's data. Unlike raw HTTP requests, snoowrap automatically handles token refreshing and rate limit tracking, which is critical given Reddit's strict limit of roughly 60 requests per minute for authenticated clients.The implementation pattern involves instantiating a snoowrap client with a refreshToken, clientId, and clientSecret. This allows the script to run indefinitely without manual re-authentication. The system should query specific subreddits relevant to the podcast's content pillars (e.g., r/science, r/upliftingnews, r/offbeat) and fetch the "Rising" or "Hot" listings.Engagement Velocity MathematicsThe raw number of upvotes is a lagging indicator. By the time a post has 50,000 upvotes, the viral wave may have already crested. To identify rising stars, the system must calculate the "Engagement Velocity" ($V$).$$V = \frac{\text{Score}}{\text{Hours Since Posted}}$$A story posted 2 hours ago with 500 upvotes ($V = 250$) is significantly more "viral" than a story posted 24 hours ago with 2000 upvotes ($V = 83$). Advanced implementations can apply a "velocity multiplier" to project future performance. For example, if the current score is high and the post is less than 4 hours old, a multiplier (e.g., 1.5x or 3x) can be applied to the velocity to account for the exponential growth phase of viral propagation.Monitoring the headers X-Ratelimit-Remaining and X-Ratelimit-Reset is mandatory. A robust system will implement a "token bucket" or exponential backoff strategy, pausing execution when the remaining requests drop below a safety threshold (e.g., 10 requests) and resuming only after the reset timestamp has passed.Natural Language Processing: The Cognitive LayerOnce potential stories are ingested and validated by social signals, the system must "read" them. This involves Natural Language Processing (NLP) to determine sentiment, extract keywords, and, most importantly for ESL, assess readability.Library Selection: Monolithic vs. ModularThe Node.js NLP ecosystem offers several distinct approaches.Natural: A comprehensive, monolithic library that includes tokenizers, stemmers (Porter, Lancaster), and classifiers (Bayes, Logistic Regression). It is excellent for general-purpose tasks but can be heavy.Compromise: Describes itself as a "crowbar for words." It is lightweight, fast, and optimized for browser-side or rapid server-side parsing. It excels at Named Entity Recognition (NER)—extracting people, places, and organizations—which is useful for tagging stories.NLP.js: The most modern and modular option (v4). It supports a plugin architecture, allowing developers to load only the necessary language packages (e.g., lang-en). It also integrates with BERT for state-of-the-art intent classification, making it a powerful choice if the system needs to categorize stories into complex themes beyond simple keywords.For a virality scoring system, a hybrid approach is often best: using compromise for fast entity extraction during the initial filtration phase, and natural or NLP.js for deeper sentiment and classification tasks on the shortlist.Sentiment Analysis and Emotional IntensitySentiment analysis is a proxy for emotional arousal, a key driver of virality. The sentiment and vader-sentiment libraries are industry standards here. VADER (Valence Aware Dictionary and sEntiment Reasoner) is particularly effective for social media text (like Reddit headlines) because it understands capitalization, punctuation (e.g., "GREAT!!!"), and emojis, all of which act as intensifiers.The scoring algorithm should not just look for "positive" or "negative" sentiment, but for intensity. A compound score near 0 indicates neutrality, which is the enemy of virality. The system should prioritize absolute values close to 1 (highly positive) or -1 (highly negative).$$I = |C|$$Where $I$ is Intensity and $C$ is the VADER compound score. Stories with $I > 0.5$ should receive a scoring boost.Pedagogical Filtering: The Flesch-Kincaid MetricThe unique requirement of this system is its educational focus. A viral story about astrophysics might be fascinating but unintelligible to a learner. The Flesch-Kincaid Grade Level formula is used to filter content:$$FKGL = 0.39 \left( \frac{\text{total words}}{\text{total sentences}} \right) + 11.8 \left( \frac{\text{total syllables}}{\text{total words}} \right) - 15.59$$Node.js libraries like flesch-kincaid or text-readability allow for the instant calculation of this metric.Beginner Podcast: Target FKGL < 5.Intermediate Podcast: Target FKGL 6–9.Advanced Podcast: Target FKGL 10+.The system should strictly discard or flag stories that exceed the target complexity, ensuring the output is always age- and skill-appropriate.The Virality Scoring EngineThe core intellectual property of this system is the algorithm that synthesizes these disparate signals into a single "Selection Score." This score determines the final playlist.The Weighted Scoring ModelWe define the Selection Score ($S_{final}$) as a weighted sum of five key components:$$S_{final} = w_1 \cdot N_{trend} + w_2 \cdot V_{social} + w_3 \cdot I_{sentiment} + w_4 \cdot R_{relevance} - w_5 \cdot P_{complexity}$$$N_{trend}$ (Normalized Trend Score): The 0-100 search interest from Google Trends. This confirms broad public interest.$V_{social}$ (Velocity Score): The calculated engagement velocity from Reddit ($Score / Hour$). This measures immediate shareability.$I_{sentiment}$ (Intensity Score): The absolute value of the sentiment compound score. High intensity correlates with higher click-through rates.$R_{relevance}$ (Domain Relevance): A boolean or tiered multiplier based on the source. "Weird News" or "Japan News" might get a 1.2x multiplier due to their proven engagement with the target audience.$P_{complexity}$ (Complexity Penalty): A deduction applied if the readability score drifts too far from the ideal range. For every grade level above the target, a penalty is subtracted.Handling ClickbaitA critical nuance is distinguishing between "engaging" and "misleading" clickbait. While clickbait headlines drive views, they can be pedagogically empty. Using a classifier trained on clickbait datasets (e.g., employing Support Vector Machines or simple heuristics like checking for "You won't believe...") allows the system to penalize deceptive clickbait while retaining curiosity-gap headlines.Implementation Strategy and Future-ProofingDatabase and PersistenceTo manage the flow of stories, a document-oriented database like MongoDB is recommended. Its schema-less nature allows for the storage of heterogeneous metadata from different APIs (e.g., Reddit's upvote_ratio vs. NewsAPI's source_id) without complex migrations. Each document should represent a story and include a time-series array of its virality score, allowing for retrospective analysis of how accurate the predictions were.Moving Toward Deep LearningWhile the heuristic model described above is effective, the state of the art involves Graph Convolutional Networks (GCNs) that model the propagation of information through a network. "ViralGCN" is an example of such a framework, predicting popularity based on the structural properties of early sharing cascades. In a future iteration of the Node.js system, the scoring engine could call out to a Python microservice hosting a pre-trained GCN model, feeding it the initial Reddit comment tree structure to get a probabilistic forecast of future shares.ConclusionThe automated virality scoring system proposed here transforms the subjective art of content curation into an objective, data-driven science. By leveraging Node.js to orchestrate the ingestion of search trends, news feeds, and social signals, and then passing this data through a rigorous NLP and pedagogical filter, the system ensures that the resulting content is not just "viral" in the general sense, but specifically optimized for the engagement and education of English learners. This architecture balances the technical constraints of API rate limits with the nuanced requirements of language education, providing a scalable solution for modern digital publishing.Data Ingestion and Source ManagementRSS Feed Configurations for Specific NichesTo operationalize the content strategy, the rss-parser configuration must be tailored to specific high-value feeds. Below is a structured approach to the feeds identified as high-potential for ESL learners.Source NameFeed URLContent NicheESL Value PropositionSoraNews24https://soranews24.com/feedJapan/Asia CultureHigh cultural interest; simple, fun narratives Oddity Centralhttps://www.odditycentral.com/feedWeird/Offbeat"Stranger than fiction" hooks; high curiosity gap Gizmodohttps://gizmodo.com/rssTechnologyModern vocab; relevance to digital life Lifehackerhttps://lifehacker.com/rssProductivity/TipsImperative verbs; actionable advice (Good for instruction) Japan Todayhttps://japantoday.com/feedCurrent Events (Japan)Local news in English; bridges L1/L2 context Good News Networkhttps://www.goodnewsnetwork.org/feedPositive NewsLow anxiety; emotional resilience topics When implementing rss-parser, standard instantiation is often insufficient for these commercial feeds which may use custom namespaces for images or full text.JavaScript// Conceptual configuration for rich data extraction
const parser = new Parser({
  customFields: {
    item: ['media:content', 'media'], // Extract high-res images from Gizmodo/Sora
      ['content:encoded', 'fullContent'], // Get full text if available
      ['dc:creator', 'author']
  }
});
This configuration ensures that the NLP layer receives the fullContent rather than just the summary, enabling more accurate readability scoring. For feeds like Lifehacker that switched to excerpt-only feeds to drive traffic , the system may need to perform a secondary fetch of the link URL using Puppeteer or Cheerio to scrape the full article text for analysis, though this increases the processing overhead.Social Media Scraping ArchitectureBeyond the official APIs, social media scraping requires a defensive architecture to maintain uptime.Reddit Scraping Strategy:
The 100 queries per minute limit  is a hard ceiling. The system should implement a "leaky bucket" algorithm. If the podcast covers 5 categories (Tech, Weird, World, Science, Japan), the scheduler must distribute these checks.Minute 0-1: Query r/technology and r/gadgets (Tech).Minute 1-2: Query r/offbeat and r/nottheonion (Weird).Minute 2-3: Query r/upliftingnews (Good News).This staggered approach ensures that the burst of requests for one category never exhausts the global rate limit. snoowrap facilitates this by returning Promise objects that can be queued. If a 429 error is encountered, the system should read the X-Ratelimit-Reset header and sleep the worker thread for exactly that duration plus a safety buffer (e.g., 2 seconds).Google Trends Scraping Strategy:
For the "Trend Matcher" module, relying solely on google-trends-api is risky due to the 429 errors. A robust fallback system uses Puppeteer.Primary Attempt: Call the @alkalisummer/google-trends-js library.Failure Handler: If a 429 is received, trigger the Puppeteer microservice.Puppeteer Execution: Launch a headless Chrome instance with puppeteer-extra-plugin-stealth. Navigate to trends.google.com/trends/trendingsearches/realtime?geo=US.Extraction: Use page.evaluate() to parse the DOM for .feed-item elements, extracting the rank, title, and traffic volume strings (e.g., "50K+ searches").Cache: Store the result in Redis with a Time-To-Live (TTL) of 60 minutes to prevent repeated scraping of the same data.Advanced NLP and Scoring LogicThe "Clickbait" Dilemma and Heuristic DetectionIn the context of an educational podcast, "clickbait" is a double-edged sword. Headlines that are too vague (e.g., "You Won't Believe What Happened Next") fail to provide the context needed for language learning. However, moderate clickbait (curiosity gaps) drives engagement.To manage this, the system should implement a "Clickbait Classifier." While deep learning models (BERT/T5) can be fine-tuned for this , a lighter heuristic approach using natural or Regex is often sufficient for a Node.js runtime:Heuristic 1: Forward Reference. Headlines starting with "This," "That," or "These" (e.g., "This Dog Did What?").Heuristic 2: Hyperbolic Adjectives. Words like "Shocking," "Mind-Blowing," "Miracle."Heuristic 3: Listicle Format. Starts with a number (e.g., "10 Reasons Why...").A "Clickbait Score" ($C_b$) can be calculated.$C_b = 0$ (Purely factual).$C_b = 1$ (Moderate curiosity).$C_b > 2$ (Aggressive clickbait).The Scoring Engine should apply a penalty for $C_b > 2$ to filter out low-quality "junk" content, while potentially boosting $C_b = 1$ stories for their engagement value.Sentiment Intensity FormulationUsing VADER (via vader-sentiment), we obtain a compound score ranging from -1.0 to 1.0. The "Intensity" used in the scoring formula must treat extreme negative and extreme positive events as equally "engaging" (high arousal).JavaScriptconst vader = require('vader-sentiment');
const intensity = vader.SentimentIntensityAnalyzer.polarity_scores(text);
// Example Output: { neg: 0.0, neu: 0.29, pos: 0.70, compound: 0.8545 }

// Calculate Intensity Component for Virality Score
let emotional_weight = Math.abs(intensity.compound); 
// 0.8545 -> Highly intense -> High viral potential
This scalar emotional_weight is then multiplied by the base virality score. A neutral story (compound ~0.05) effectively dampens the total score, ensuring that dry, emotionless news is filtered out.Pedagogical Filtering AlgorithmsFlesch-Kincaid IntegrationThe pedagogical filter is the final gatekeeper. Using the text-readability package, we can automate the grade-level assessment.JavaScriptconst readability = require('text-readability');
const score = readability.fleschKincaidGrade(fullArticleText);
For a "Daily English Learning Podcast," we can define dynamic bands:Band A (Beginner): FKGL < 6. Priority given to "Good News" and "Weird News" sources which tend to use simpler language.Band B (Intermediate): FKGL 6-10. Priority given to "Tech" (Gizmodo) and "Japan" (SoraNews24) sources.Band C (Advanced): FKGL > 10. Priority given to "World News" (BBC/Reuters) or complex "Science" articles.The system can automatically tag each viral story with its "Level" (A, B, or C). If the podcast is strictly for intermediate learners, any story with FKGL > 12 is discarded regardless of its viral score, or marked for "simplification" (a flag for the scriptwriters to rewrite the text).Vocabulary Density AnalysisBeyond simple grade levels, the system can use natural to analyze vocabulary density. By comparing the article's tokens against a "Stopword" list (common words like the, is, at) and a "Frequency List" (e.g., the Oxford 3000), the system can calculate a "Lexical Density" score.High Lexical Density = Many unique, content-carrying words (Harder).Low Lexical Density = Repetitive, simple structure (Easier).This provides a secondary filter. A story might have short sentences (low FKGL) but obscure scientific jargon (high complexity). The Lexical Density check catches these edge cases.ConclusionThe architecture defined here moves beyond simple aggregation to intelligent curation. By synthesizing Google Trends for macro-interest, Reddit for micro-velocity, and NLP for linguistic suitability, this Node.js system acts as an automated editor-in-chief. It balances the conflicting demands of the "attention economy" (virality) and the "education economy" (pedagogy), delivering a stream of content that is engaging enough to click but substantial enough to teach. The use of resilient design patterns—stealth scraping, exponential backoff, and event-driven queues—ensures that the system remains operational in the face of the increasingly hostile bot-detection landscape of the modern web.

---

## Claude Notes
_This section will be filled by Claude after reviewing the research._

- Status: Pending review
- Key findings: [to be filled]
- Action items: [to be filled]
